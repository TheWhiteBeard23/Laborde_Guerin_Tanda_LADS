{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Rendu_DEFIIA_GUERIN_LABORDE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8iZrHj-3Gut"
      },
      "source": [
        "<u><center><h1> Projet IA </h1></center></u>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paU6AD803Guz"
      },
      "source": [
        "<img\n",
        "    src=\"https://media.lesechos.com/api/v1/images/view/5e56479f3e45463f2c5e2f78/1280x720-webp/11513-1522338774-ia.webp\"\n",
        "    alt=\"[ABC Tech posssède 75% de part de marché et XYZ 25%]\"\n",
        "    height=\"600px\"\n",
        "    width=\"600px\"\n",
        "/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryF9blN03Gu0"
      },
      "source": [
        "<u> <h2><FONT color=\"darkred\"> Préambule </FONT></h2> </u>\n",
        "    \n",
        "<h3>Point d'attention</h3>\n",
        "    \n",
        "Tous les codes adaptés de codes mis à disposition sur internet sont cités dans la bibliographie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXfGp2aI3Gu1"
      },
      "source": [
        "<h3> a) Importation des packages </h3>\n",
        "\n",
        "<p style=\"text-align: justify;\">Ici, on importe tous les packages nécessaires à l'utilisation des différentes techniques présentées dans le cadre ce rapport. Sklearn ainsi que nltk sont deux packages très répandus lors de la réalisation de travaux sur le domaine NLP (Natural processing language).</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJEF77Tv3Gu2",
        "outputId": "a34b8757-6881-4f15-9fba-2137d46d00b0"
      },
      "source": [
        "#Package\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# les installations de packages\n",
        "\n",
        "#!pip install langdetect\n",
        "#!pip install pyquickhelper\n",
        "#!pip install pyensae\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import wordnet \n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "from translate import Translator\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "from spacy.cli import download\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "#Declaration d'une fonction pour la suite\n",
        "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "def tokenize(s): return re_tok.sub('', s).split()\n",
        "\n",
        "#Dowload\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\cleme\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\cleme\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\cleme\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUTUkGmj3Gu5"
      },
      "source": [
        "<h3> b) Importation des données </h3>\n",
        "\n",
        "<p style=\"text-align: justify;\">Dans cette section les données sont importées. Cela est fait depuis deux fichiers \".json\" pour les 217 000 descriptions de la base de données d'apprentissage ainsi que les 54 000 descriptions de la base données test. On importe un \".csv\" contenant les classes auxquelles appartiennent les 217 000 descriptions d'apprentissage en plus.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oelVl9XQ3Gu7"
      },
      "source": [
        "train_df = pd.read_json(\"train.json\")\n",
        "test_df = pd.read_json(\"test.json\")\n",
        "train_label = pd.read_csv(\"train_label.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vv7cFAD3Gu7"
      },
      "source": [
        "<u> <h2><FONT color=\"darkred\"> I - Quelques statistiques descriptives </FONT></h2> </u>\n",
        "    \n",
        "<p style=\"text-align: justify;\">Afin de mieux comprendre notre jeu de données on peut l'étudier à l'aide de quelques statistiques descriptives. Dans un premier temps on étudie les longueurs moyennes, les écarts-types, les quartiles, etc... de la longueur de nos descriptions.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDUT_ZrD3Gu7",
        "outputId": "f56ac686-b8c2-45c2-9c48-c761ed779f27"
      },
      "source": [
        "#apprentissage\n",
        "description_text = train_df.description\n",
        "description_token = description_text.apply(tokenize)\n",
        "\n",
        "vect=[]\n",
        "for line in description_token:\n",
        "    vect.append(len(line))\n",
        "        \n",
        "#test\n",
        "description_text = test_df.description\n",
        "description_token = description_text.apply(tokenize)\n",
        "\n",
        "vect_test=[]\n",
        "for line in description_token:\n",
        "    vect_test.append(len(line))\n",
        "    \n",
        "vect_1=pd.DataFrame(vect)\n",
        "vect_2=pd.DataFrame(vect_test)\n",
        "vect_1 = vect_1.rename({0: 'Apprentissage'}, axis=1)\n",
        "vect_2 = vect_2.rename({0: 'Test'}, axis=1)\n",
        "\n",
        "stat_des = pd.concat([vect_1, vect_2], axis=1)\n",
        "stat_des.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Apprentissage</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>217197.000000</td>\n",
              "      <td>54300.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>61.039959</td>\n",
              "      <td>60.973775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>28.022422</td>\n",
              "      <td>28.084658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>56.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>79.000000</td>\n",
              "      <td>79.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>179.000000</td>\n",
              "      <td>174.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Apprentissage          Test\n",
              "count  217197.000000  54300.000000\n",
              "mean       61.039959     60.973775\n",
              "std        28.022422     28.084658\n",
              "min         6.000000      8.000000\n",
              "25%        38.000000     38.000000\n",
              "50%        56.000000     56.000000\n",
              "75%        79.000000     79.000000\n",
              "max       179.000000    174.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtk93W0Q3Gu8"
      },
      "source": [
        "<p style=\"text-align: justify;\">On observe de ce côté-là une très grande similitude entre les jeux de données de test et d'apprentissage concernant les descriptions qui les composent. Il n'y a donc à ce niveau là pas de raison d'envisager d'effectuer un retraitement pour les rendre plus similaires. On peut aussi observer ces répartitions de manières plus précises avec des histogrammes. L'histogramme rouge pour le jeu d'apprentissage et le bleu pour le test. Les deux sont ainsi très similaires.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vij9_H9g3Gu9",
        "outputId": "61477d2c-bc1f-4adc-aa0f-079aec08d9d7"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2)\n",
        "\n",
        "ax1.hist(vect,bins=10,color='r',alpha=0.7, rwidth=0.85,density=False)\n",
        "ax1.grid(True)\n",
        "ax2.hist(vect_test,bins=10,color='b',alpha=0.7, rwidth=0.85,density=False)\n",
        "ax2.grid(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfmWI98F3Gu-"
      },
      "source": [
        "On peut aussi s'intéresser à la répartition des effectifs par classe ainsi que de la répartition homme/femme à l'intérieur de chaque classe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB80ZkiP3Gu-",
        "outputId": "95ba38ff-7353-48af-c18b-b978c76bb720"
      },
      "source": [
        "# Catégorie professionnelle et répartition homme/femme\n",
        "names = pd.read_csv('categories_string.csv')['0'].to_dict()\n",
        "jobs = pd.read_csv('train_label.csv', index_col='Id')['Category']\n",
        "jobs = jobs.map(names)\n",
        "jobs = jobs.rename('job')\n",
        "genders = pd.read_json('train.json').set_index('Id')['gender']\n",
        "\n",
        "people = pd.concat((jobs, genders), axis='columns')\n",
        "counts = people.groupby(['job', 'gender']).size().unstack('gender')\n",
        "counts\n",
        "# Représentation graphique de la répartition des métiers par sexe\n",
        "nb_jobs = len(counts)\n",
        "\n",
        "metier = np.sort(list(names.values()))\n",
        "sexe =['Hommes','Femmes']\n",
        "pos = np.arange(len(metier))\n",
        "metier_hommes = list(counts.M)\n",
        "metier_femmes = list(counts.F)\n",
        "\n",
        "plt.figure(figsize=(10,15))\n",
        "plt.barh(pos,metier_hommes,color='blue',edgecolor='black')\n",
        "plt.barh(pos,metier_femmes,color='orange',edgecolor='black',left=metier_hommes)\n",
        "plt.yticks(pos, metier)\n",
        "plt.ylabel('Métier', fontsize=20)\n",
        "plt.xlabel('Effectifs', fontsize=16)\n",
        "plt.title('Répartition hommes/femmes par métier',fontsize=18)\n",
        "plt.legend(sexe,loc=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8u95Cox3Gu-"
      },
      "source": [
        "<p style=\"text-align: justify;\">Cette représentation est extrêmement informative et permet d'observer un énorme déséquilibre dans les classes. Les professeurs représentent plus de 70 000 descriptions soit près d'un tiers alors que les professeurs de yoga, coachs personnels ou rappeurs ne sont que très peu représentés. Ceci peut causer des déséquilibres pour la suite si les algorithmes ne sont pas assez performants pour reconnaitre les différences entre les descriptions des métiers, ils peuvent avoir tendance à surreprésenter les classes majoritaires (souvent la classe 19 : professeur).</p>\n",
        "    \n",
        "<p style=\"text-align: justify;\">Il existe des méthodes pour contrebalancer ce problème partiellement si le jeu de données d'apprentissage est très différent du jeu de données test dans la répartition des classes. Cependant, n'ayant pas les labels du jeu test il est impossible de le vérifier. Nous avons mis en place certaines de ces techniques (l'une consiste à dupliquer certaines descriptions des classes minoritaires, l'autre consiste à traduire certaines descriptions des classes minoritaires vers une langue tierce (par exemple l'allemand ou le français) avant de retraduire dans la langue anglaise.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Dans cette deuxième technique le résultat est ainsi légèrement différent de l'original et on a ainsi une autre description de la classe. Cependant aucune de ces deux méthodes ne s'est avérée efficace traduisant potentiellement que la répartition des classes dans le jeu de données test est sans doute très proche du jeu de données d'apprentissage. Nous mettons tout de même ci-dessous un code permettant de détecter la langue d'origine d'une description ainsi que de la traduire vers une langue cible. Nous avons mené cette étude mais celle-ci n'étant pas concluante nous n'irons pas plus loin.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3vHY9IH3Gu_",
        "outputId": "77e520e9-705a-4c85-84e8-3ea7f9f37fc0"
      },
      "source": [
        "translator= Translator(to_lang=\"en\",from_lang = \"fr\")\n",
        "translation = translator.translate('Ceci est un test afin de faire une illustration convaincante.')\n",
        "print(translation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a test in order to make a convincing illustration.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDCPS7Fd3GvA"
      },
      "source": [
        "<p style=\"text-align: justify;\">On a aussi étudié la langue des descriptions mais seulement un nombre largement minoritaire dans l'ensemble d'apprentissage et de test sont dans une langue différente de l'anglais (116 pour l'apprentissage et 28 pour le test). Un exemple d'une de ces descriptions ci-dessous avec un code pour en détecter la langue :</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6xLSMno3GvA",
        "outputId": "02a5206b-68d9-4b5f-ca94-21b5d85a8d3e"
      },
      "source": [
        "lang=detect(train_df.iloc[267][\"description\"])\n",
        "print(train_df.iloc[267][\"description\"])\n",
        "print(\" \")\n",
        "print(\"cette description est en \"+\"\\\"\"+lang+\"\\\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " He informs martial arts to his two children. However, his past catches up to him because a scoundrel’s agent requires to know the whereabouts of an agent known as a dragon. Now father and children must get together to stop the scoundrel’s agents and his idiots. Teddy Yu ist gedrehter Chiropraktiker eines ehemaligen Geheimagenten, der dachte, dass er seine Vergangenheit zurückließ. Er unterrichtet Kampfsportarten seinen zwei Kindern. Jedoch holt seine Vergangenheit zu ihm auf, weil ein Schelm-Agent verlangt, den Verbleib eines als Drache bekannten Agenten zu wissen. Jetzt müssen sich Vater und Kinder zusammentun, den Schelm-Agenten und seine Idioten aufzuhören.\n",
            " \n",
            "cette description est en \"de\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZYcviRj3GvA"
      },
      "source": [
        "D'autres statistiques descriptives ont aussi été effectuées lors des différents cours en classe provenant du code de Mr BIGOT, comme des word clouds (nuage de mots) ou autres. Cependant nous n'avons pas jugé intéressant de les faire apparaître ici, étant donné que notre meilleure méthode, BERT, que nous introduirons par la suite n'est pas vraiment sensible à ces études préliminaires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYc48OK23GvA"
      },
      "source": [
        "<u> <h2><FONT color=\"darkred\"> II - Prétraitement des données </FONT></h2> </u>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_jXILfg3GvB"
      },
      "source": [
        "<p style=\"text-align: justify;\">Dans un premier temps pour de nombreuses méthodes (pas toutes, on verra que BERT ne le requiert pas par exemple) on effectue un prétraitement de données.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">La première étape consiste à mettre les descriptions en minuscules afin que par la suite deux mots ne soient pas identifiés comme étant diffèrent si l'un est le même que l'autre se situant seulement au début d'une phrase. Exemple:\n",
        "\"study\" et \"Study\".</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHLDzhh23GvB"
      },
      "source": [
        "# Description en minuscule\n",
        "\n",
        "train_df[\"description_lower\"] = [x.lower() for x in train_df.description]\n",
        "test_df[\"description_lower\"] = [x.lower() for x in test_df.description]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td93_H2P3GvB"
      },
      "source": [
        "<h3> a) Lemmatization </h3>\n",
        "\n",
        "<p style=\"text-align: justify;\">La lemmatization est une étape souvent très importante lors de l'étude d'un problème de NLP. Elle consiste à rapporter des mots à une racine. Ainsi les mots \"playing\", \"played\" et \"plays\" sont ramenés à la même racine \"play\" par exmple et seront identifiés comme identiques par la suite. Cela participe en général à améliorer grandement les résultats. De plus des verbes issus de la même base y seront ramenés comme \"was\", \"were\", \"been\" etc... seront ramenés à \"be\".</p>\n",
        "<p style=\"text-align: justify;\">Pour finir, il convient de noter qu'il existe différentes fonctions pour lemmatizer (package spacy, stanford, gensim, pattern etc...). Nous en avons étudié plusieurs et celle de WordNetLemmatizer associée à des POS_TAG (petites lettres signifiant si le mot est un verbe, un nom, un adverbe etc...) est celle qui nous a fourni les meilleurs résultats.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uQGoAzz3GvB"
      },
      "source": [
        "<h4> Lemmatization de l'ensemble d'entrainement </h4>\n",
        "\n",
        "<p style=\"text-align: justify;\">Ci-dessous on effectue la lemmatization du jeu de données d'apprentissage. On affiche de plus des exemples pour observer les résultats.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "702mj_T-3GvC"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer() \n",
        "def pos_tagger(nltk_tag): \n",
        "    if nltk_tag.startswith('J'): \n",
        "        return wordnet.ADJ \n",
        "    elif nltk_tag.startswith('V'): \n",
        "        return wordnet.VERB \n",
        "    elif nltk_tag.startswith('N'): \n",
        "        return wordnet.NOUN \n",
        "    elif nltk_tag.startswith('R'): \n",
        "        return wordnet.ADV \n",
        "    else:           \n",
        "        return None\n",
        "\n",
        "# Lemmatisation du jeu de données train\n",
        "vect=[]\n",
        "for i in train_df[\"description_lower\"]:\n",
        "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(i))  \n",
        "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged)) \n",
        "\n",
        "    lemmatized_sentence = \" \"\n",
        "    for word, tag in wordnet_tagged: \n",
        "        if tag is None:  \n",
        "            lemmatized_sentence=lemmatized_sentence+\" \"+word \n",
        "        else:         \n",
        "            lemmatized_sentence=lemmatized_sentence+\" \"+lemmatizer.lemmatize(word, tag)\n",
        "    vect.append(lemmatized_sentence)\n",
        "train_df[\"description_lemmatized\"]=vect\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhzoDSvk3GvC",
        "outputId": "22487876-ac31-484b-bb5a-8a20b926ba35"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>description</th>\n",
              "      <th>gender</th>\n",
              "      <th>description_lower</th>\n",
              "      <th>description_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>She is also a Ronald D. Asmus Policy Entrepre...</td>\n",
              "      <td>F</td>\n",
              "      <td>she is also a ronald d. asmus policy entrepre...</td>\n",
              "      <td>she be also a ronald d. asmus policy entrepr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>He is a member of the AICPA and WICPA. Brent ...</td>\n",
              "      <td>M</td>\n",
              "      <td>he is a member of the aicpa and wicpa. brent ...</td>\n",
              "      <td>he be a member of the aicpa and wicpa . bren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Dr. Aster has held teaching and research posi...</td>\n",
              "      <td>M</td>\n",
              "      <td>dr. aster has held teaching and research posi...</td>\n",
              "      <td>dr. aster have hold teach and research posit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>He runs a boutique design studio attending cl...</td>\n",
              "      <td>M</td>\n",
              "      <td>he runs a boutique design studio attending cl...</td>\n",
              "      <td>he run a boutique design studio attend clien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>He focuses on cloud security, identity and ac...</td>\n",
              "      <td>M</td>\n",
              "      <td>he focuses on cloud security, identity and ac...</td>\n",
              "      <td>he focus on cloud security , identity and ac...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id                                        description gender  \\\n",
              "0   0   She is also a Ronald D. Asmus Policy Entrepre...      F   \n",
              "1   1   He is a member of the AICPA and WICPA. Brent ...      M   \n",
              "2   2   Dr. Aster has held teaching and research posi...      M   \n",
              "4   3   He runs a boutique design studio attending cl...      M   \n",
              "5   4   He focuses on cloud security, identity and ac...      M   \n",
              "\n",
              "                                   description_lower  \\\n",
              "0   she is also a ronald d. asmus policy entrepre...   \n",
              "1   he is a member of the aicpa and wicpa. brent ...   \n",
              "2   dr. aster has held teaching and research posi...   \n",
              "4   he runs a boutique design studio attending cl...   \n",
              "5   he focuses on cloud security, identity and ac...   \n",
              "\n",
              "                              description_lemmatized  \n",
              "0    she be also a ronald d. asmus policy entrepr...  \n",
              "1    he be a member of the aicpa and wicpa . bren...  \n",
              "2    dr. aster have hold teach and research posit...  \n",
              "4    he run a boutique design studio attend clien...  \n",
              "5    he focus on cloud security , identity and ac...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipx8k1KS3GvC"
      },
      "source": [
        "Les descriptions ont bien été lemmatizées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sV-IaqU3GvD"
      },
      "source": [
        "<h4>Lemmatization du jeu de données test</h4>\n",
        "\n",
        "Ci-dessous on effectue la lemmatization du jeu de données test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1vEyTMx3GvD"
      },
      "source": [
        "# Lemmatisation du jeu de données test\n",
        "\n",
        "vect=[]\n",
        "for i in test_df[\"description_lower\"]:\n",
        "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(i))\n",
        "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged)) \n",
        "    lemmatized_sentence = \" \"\n",
        "    for word, tag in wordnet_tagged: \n",
        "        if tag is None: \n",
        "            lemmatized_sentence=lemmatized_sentence+\" \"+word \n",
        "        else:         \n",
        "            lemmatized_sentence=lemmatized_sentence+\" \"+lemmatizer.lemmatize(word, tag)\n",
        "\n",
        "    vect.append(lemmatized_sentence)\n",
        "test_df[\"description_lemmatized\"]=vect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkcJhART3GvE"
      },
      "source": [
        "<u><h3> b) Les stop words </h3></u>\n",
        "\n",
        "<p style=\"text-align: justify;\">Une autre étape est d'étudier l'impact des stop words et plus particulièrement l'impact de leur présence ou non. Un stop word est un mot très (trop) présent dans le vocabulaire anglais et qui n'aurait ainsi aucun pouvoir discriminant. Par exemple les mots \"the\", \"of\", \"a\" ,\"I\" etc.. sont présents dans une grande majorité de phrases et ne sont donc pas discriminants. Ainsi, on peut supposer que les enlever de nos descriptions présentera un avantage (hormis pour BERT).</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Cependant l'efficacité de cette technique est très dépendante de l'étape suivante : la représentation de nos mots. Nous verrons par la suite que la méthode qui est en général la plus efficace dans une majorité de problèmes de NLP et qui l'est aussi ici, le TF-IDF, absorbe une grosse partie de l'intérêt d'enlever les stop words, voir que cela peut etre légèrement contre-productif. Nous développerons pourquoi dans la partie liée au TF-IDF. Quoi qu'il en soit le code ci-dessous permet d'aller chercher une liste de mots stop words en anglais et de les retirer de nos descriptions.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7V2vS9A3GvE"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#stopwords\n",
        "description_text2 = test_df.description_lemmatized\n",
        "test_df[\"description_lemmatized2\"]=test_df[\"description_lemmatized\"]\n",
        "i=0\n",
        "for line in description_text2:  \n",
        "    word_tokens = word_tokenize(line)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = []\n",
        "    sent=\"\"\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "            sent=sent+str(w)+\" \"\n",
        "    test_df[\"description_lemmatized2\"][i:i+1] = sent\n",
        "    i=i+1\n",
        "    \n",
        "#stopwords\n",
        "description_text2 = train_df.description_lemmatized\n",
        "train_df[\"description_lemmatized2\"]=train_df[\"description_lemmatized\"]\n",
        "i=0\n",
        "for line in description_text2:  \n",
        "    word_tokens = word_tokenize(line)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = []\n",
        "    sent=\"\"\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "            sent=sent+str(w)+\" \"\n",
        "    train_df[\"description_lemmatized2\"][i:i+1] = sent\n",
        "    i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBcgP9Q3GvE"
      },
      "source": [
        "<p style=\"text-align: justify;\">A l'issue de cette étape les descriptions sont lemmatizées et sans stop words. on peut voir des exemples ci-dessous où la colonne description_lemmatized2 est composée des descriptions lemmatizées et sans stop words :</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY2UOKdE3GvE",
        "outputId": "724dff60-5ec8-4968-f76f-3852c502083b"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>description</th>\n",
              "      <th>gender</th>\n",
              "      <th>description_lower</th>\n",
              "      <th>description_lemmatized</th>\n",
              "      <th>description_lemmatized2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>She is also a Ronald D. Asmus Policy Entrepre...</td>\n",
              "      <td>F</td>\n",
              "      <td>she is also a ronald d. asmus policy entrepre...</td>\n",
              "      <td>she be also a ronald d asmus policy entreprene...</td>\n",
              "      <td>also ronald asmus policy entrepreneur fellow g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>He is a member of the AICPA and WICPA. Brent ...</td>\n",
              "      <td>M</td>\n",
              "      <td>he is a member of the aicpa and wicpa. brent ...</td>\n",
              "      <td>he be a member of the aicpa and wicpa brent gr...</td>\n",
              "      <td>member aicpa wicpa brent graduate university w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Dr. Aster has held teaching and research posi...</td>\n",
              "      <td>M</td>\n",
              "      <td>dr. aster has held teaching and research posi...</td>\n",
              "      <td>dr aster have hold teach and research position...</td>\n",
              "      <td>dr aster hold teach research position ben guri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>He runs a boutique design studio attending cl...</td>\n",
              "      <td>M</td>\n",
              "      <td>he runs a boutique design studio attending cl...</td>\n",
              "      <td>he run a boutique design studio attend clients...</td>\n",
              "      <td>run boutique design studio attend clients unit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>He focuses on cloud security, identity and ac...</td>\n",
              "      <td>M</td>\n",
              "      <td>he focuses on cloud security, identity and ac...</td>\n",
              "      <td>he focus on cloud security identity and access...</td>\n",
              "      <td>focus cloud security identity access managemen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id                                        description gender  \\\n",
              "0   0   She is also a Ronald D. Asmus Policy Entrepre...      F   \n",
              "1   1   He is a member of the AICPA and WICPA. Brent ...      M   \n",
              "2   2   Dr. Aster has held teaching and research posi...      M   \n",
              "4   3   He runs a boutique design studio attending cl...      M   \n",
              "5   4   He focuses on cloud security, identity and ac...      M   \n",
              "\n",
              "                                   description_lower  \\\n",
              "0   she is also a ronald d. asmus policy entrepre...   \n",
              "1   he is a member of the aicpa and wicpa. brent ...   \n",
              "2   dr. aster has held teaching and research posi...   \n",
              "4   he runs a boutique design studio attending cl...   \n",
              "5   he focuses on cloud security, identity and ac...   \n",
              "\n",
              "                              description_lemmatized  \\\n",
              "0  she be also a ronald d asmus policy entreprene...   \n",
              "1  he be a member of the aicpa and wicpa brent gr...   \n",
              "2  dr aster have hold teach and research position...   \n",
              "4  he run a boutique design studio attend clients...   \n",
              "5  he focus on cloud security identity and access...   \n",
              "\n",
              "                             description_lemmatized2  \n",
              "0  also ronald asmus policy entrepreneur fellow g...  \n",
              "1  member aicpa wicpa brent graduate university w...  \n",
              "2  dr aster hold teach research position ben guri...  \n",
              "4  run boutique design studio attend clients unit...  \n",
              "5  focus cloud security identity access managemen...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDWTWfgF3GvE"
      },
      "source": [
        "On observe que des mots extrêmement communs en anglais comme les pronoms ou \"the\", \"of\" etc... ne sont simplement plus présent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTsbzzov3GvE"
      },
      "source": [
        "<u> <h2><FONT color=\"darkred\"> III - Représentations des mots </FONT></h2> </u>\n",
        "\n",
        "<p style=\"text-align: justify;\">Dans un second temps, après le prétraitement des données, il faut choisir une méthode pour représenter nos descriptions. Plusieurs techniques existent comme les Document Term Matrix (DTM), les Term Frequency - Inverse Document Frequency (TF-IDF), les Word to Vec (W2V) et bien d'autres. Souvent, la représentation TF-IDF permet d'obtenir parmi les meilleurs résultats. Ici nous avons étudié les 3 différentes méthodes afin de les comparer. Nous étudierons les résultats dans la partie suivante, dans cette partie nous étudierons seulement les codes pour les mettre en place et nous décrirons brièvement leurs principes.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IICpVhe3GvE"
      },
      "source": [
        "<h3> TF-IDF </h3>\n",
        "\n",
        "<p style=\"text-align: justify;\">La représentation TF-IDF est l'une des plus répandus et produit en général de bons résultats. Il s'agit d'une matrice composée d'autant de lignes qu'il y a de descriptions (documents) et d'autant de colonnes qu'il y a de mots différents dans l'ensemble de ces documents réunis (termes). Pour la construire il faut : compter le nombre d'occurrences de chaque mots dans l'ensemble des documents réunit ainsi que le nombre d'occurence de chaque mots dans chaque description prise une par une.</p>\n",
        "- Ensuite on calcule idf_i=log(D/N) avec D le nombre de documents total du corpus et N le nombre de documents où le terme i apparait.\n",
        "- On calcule tf_i,j= Nombre de fois où le terme i apparait dans le document / Nombre total de termes dans le document.\n",
        "\n",
        "Ensuite on associe à la ligne i et à la colonne j : tfidf_i,j= tf_i,j*idf_i.\n",
        "\n",
        "<p style=\"text-align: justify;\">Les valeurs les plus élevées sont ainsi le reflet du fait que certains mots soient caractéristiques de certains documents et non de tous. On comprend ainsi facilement que les stop words, présents dans une majorité de documents ne seront au final caractéristique de personne. Leur présence ou non importe donc peu ici.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LRTzNZI3GvF",
        "outputId": "91d89d28-d934-4f98-9e5d-9a8b537dddfd"
      },
      "source": [
        "# TF-IDF\n",
        "transformer = TfidfVectorizer().fit(train_df[\"description_lemmatized\"].values)\n",
        "print(\"NB features: %d\" %(len(transformer.vocabulary_)))\n",
        "X_train = transformer.transform(train_df[\"description_lemmatized\"].values)\n",
        "X_test = transformer.transform(test_df[\"description_lemmatized\"].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB features: 220300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5H0eQC13GvF"
      },
      "source": [
        "<p style=\"text-align: justify;\">On observe ici qu'après l'application du TF-IDF nos données sont en dimension 220 300, ce qui est inférieur à la dimension initiale (le nombre de mots différents à l'origine) qui était de 230 000.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAYvstM3GvG"
      },
      "source": [
        "<h3> Word2Vec </h3>\n",
        "\n",
        "<p style=\"text-align: justify;\">La représentation Word2Vec est aussi très utilisée. Il s'agit d'une matrice composée d'autant de lignes qu'il y a de documents différents dans le corpus et d'un nombre de colonnes prédéfinis à l'avance. Dans cette représentation chaque mot est projeté dans un sous espace de plus petite dimension afin de réduire la taille des matrices. On définit une notion de distance et les mots les plus proches sémantiquement parlant sont d'autant plus proche en terme de distance dans cette représentation. En général on projette les mots dans un espace de dimension entre 100 et 300. La représentation d'une description est alors seulement la somme des représentations vectorielles des mots la composant ou parfois quelque chose de plus fin que nous ne détaillerons pas ici.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Ainsi, en prenant un exemple proche du notre, dans le cas de 200 000 documents composés de 200 000 mots différents, en projetant les mots dans un sous espace de dimension 300 on peut passer d'une matrice de taille 40 * 10^9 à une matrice de taille 60 * 10^6. Ce gain est énorme. Cependant il s'avère pour la suite que cette représentation est assez couteuse à calculer et ne produit pas des résultats aussi bons que les autres.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVa1qV7t3GvG",
        "outputId": "1393a5f6-84f4-4d3e-b9a5-aada1340764e"
      },
      "source": [
        "import os\n",
        "version = \"2.2.0\"\n",
        "unzip_dest = 'en_core_web_md-{0}.tar/dist/en_core_web_md-{0}/en_core_web_md/en_core_web_md-{0}'.format(version)\n",
        "if not os.path.exists(unzip_dest):\n",
        "    from pyquickhelper.pycode import is_travis_or_appveyor\n",
        "    if not is_travis_or_appveyor():\n",
        "        # On le fait seulement si ce n'est pas un test d'intégration continue.\n",
        "        url = \"https://github.com/explosion/spacy-models/releases/download/en_core_web_md-%s/\" % version\n",
        "        name = \"en_core_web_md-%s.tar.gz\" % version\n",
        "        print(\"Téléchargement de \", name)\n",
        "        from pyensae.datasource import download_data\n",
        "        unzipped = download_data(name, url=url, fLOG=print)\n",
        "        unzip_dest = os.path.split(unzipped[0])[0]\n",
        "        unzip_dest = \"en_core_web_md-{0}/en_core_web_md/en_core_web_md-{0}\".format(version)\n",
        "        print(\"Found\", unzip_dest)\n",
        "\n",
        "if os.path.exists(unzip_dest):\n",
        "    print(\"Chargement des données par spacy.\")\n",
        "    nlp = spacy.load(unzip_dest)\n",
        "    continue_wv = True\n",
        "else:\n",
        "    continue_wv = False\n",
        "    print('Pas de données on passe la suite.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Téléchargement de  en_core_web_md-2.2.0.tar.gz\n",
            "Found en_core_web_md-2.2.0/en_core_web_md/en_core_web_md-2.2.0\n",
            "Chargement des données par spacy.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqMgZV4I3GvG",
        "outputId": "2e3aef3f-399f-4633-a161-9c041a0f77ae"
      },
      "source": [
        "import numpy\n",
        "\n",
        "def spacy_sum_vectors(phrase, nlp):\n",
        "    dec = nlp(phrase)\n",
        "    return sum(w.vector for w in dec)\n",
        "\n",
        "def spacy_word2vec_features(X, nlp):\n",
        "    feats = numpy.vstack([spacy_sum_vectors(p, nlp) for p in X])\n",
        "    return feats\n",
        "\n",
        "if continue_wv:\n",
        "    try:\n",
        "        X_train_W2V = spacy_word2vec_features(train_df['description_lemmatized'], nlp)\n",
        "        print(X_train_W2V.shape)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        continue_wv = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(217197, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ALwFpU3GvH"
      },
      "source": [
        "On a bien ici que les différents mots et donc les différentes descriptions ont été envoyés dans un espace de plus petite dimension (300).\n",
        "\n",
        "On s'occupe ensuite du jeu de test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epKAHvLU3GvH"
      },
      "source": [
        "if continue_wv:\n",
        "    try:\n",
        "        X_test_W2V = spacy_word2vec_features(test_df['description_lemmatized'], nlp)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        continue_wv = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNBCynW43GvH"
      },
      "source": [
        "<h3>DTM</h3>\n",
        "\n",
        "<p style=\"text-align: justify;\">Une autre représentation est la Document Term Matrix, qui est composée d'autant de lignes qu'il y a de documents et d'autant de colonnes qu'il y a de mots différents. Cette représentation consiste simplement à affecter à la ligne i et à la colonne j le nombre d'apparitions du mot j dans le document i. Cette représentation est moins travaillée que TF-IDF et est en général moins efficace. On peut observer la dimension de la matrice résultante ci-dessous :</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDEql2693GvH",
        "outputId": "b63c8b28-3887-4657-ab33-f46dbb8d89c0"
      },
      "source": [
        "X_train_DTM = CountVectorizer().fit_transform(train_df[\"description_lemmatized\"].values)\n",
        "X_train_DTM.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(217197, 220300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8m1ps213GvI"
      },
      "source": [
        "X_test_DTM = CountVectorizer().fit_transform(test_df[\"description_lemmatized\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDHNZWIQ3GvI"
      },
      "source": [
        "<u> <h2><FONT color=\"darkred\"> IV - Les différentes méthodes étudiées </FONT></h2> </u>\n",
        "\n",
        "<p style=\"text-align: justify;\">Maintenant que les données ont été prétraitées et que des représentations de mots ont été choisies, nous allons étudier différents algorithmes pour classifier nos descriptions. Afin de tester la qualité des algorithmes employés nous effectuerons un découpage apprentissage/validation à l'intérieur du jeu d'apprentissage. Nous apprendrons sur le premier composé d'environ 70% des données d'apprentissage soit environ 150 000 descriptions et nous étudierons la qualité des résultats sur les 30% restants.</p>\n",
        "    \n",
        "<p style=\"text-align: justify;\">La métrique pour mesurer la qualité des résultats sera simplement le pourcentage de prédictions correctes. Il convient de noter pour toute la suite un élément extrêmement important ! Lors d'une soumission sur le kaggle du défi on observe un décalage avec le score de manière permanente d'environ 5 à 5,5%. Cependant sur les 25 soumissions effectuées la hiérachie des résultats des méthodes est toujours restée la même entre l'échantillon de validation et la soumission kaggle. Nous interpréterons les résultats obtenus sur ce notebook en conséquence mais une méthode obtenant de meilleurs résultats ici obtient aussi de meilleurs résultats sur le defia ia et avec un écart de performances très comparables.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtBwTD4c3GvI"
      },
      "source": [
        "<h4> Découpage Apprentissage / Validation</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGjBprX43GvJ"
      },
      "source": [
        "#Y_train = train_label.Category.values\n",
        "#train_x,test_x,train_y,test_y = train_test_split(X_train,Y_train,test_size = 0.99)\n",
        "\n",
        "X_train_split_DTM,X_valid_split_DTM,y_train_split_DTM,y_valid_split_DTM = train_test_split(X_train_DTM,train_label.Category.values,test_size = 0.3, random_state=42)\n",
        "X_train_split_TFID,X_valid_split_TFID,y_train_split_TFID,y_valid_split_TFID = train_test_split(X_train,train_label.Category.values,test_size = 0.3, random_state=42)\n",
        "X_train_split_W2V,X_valid_split_W2V,y_train_split_W2V,y_valid_split_W2V = train_test_split(X_train_W2V,train_label.Category.values,test_size = 0.3, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfRP7vp23GvJ"
      },
      "source": [
        "<p style=\"text-align: justify;\">Nous allons donc dans une première partie étudier les méthodes de Régression logistique, SVM, des forêts aléatoires, du SGDC et Naive Bayes. Nous avons aussi étudier XGB, Bernoulli Naive Bayes et les KNN mais ils ne produisent pas de bons résultats et sont longs à faire tourner, pour des soucis de lisibilité nous les avons donc enlevé. Pour la régression logistique nous étudierons une alternative avec de la validaion croisée. Toutes ces méthodes seront étudiées de manière succincte pour présenter seulement leurs résultats afin de présenter plus en profondeur par la suite la méthode qui les dépasse toutes par une marge significative : BERT</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeHxBMbL3GvJ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "nb = X_train_split_TFID.shape[0]\n",
        "\n",
        "#Reression Logistique\n",
        "model_DTM_RL = LogisticRegression(solver='saga')\n",
        "model_TFID_RL = LogisticRegression(solver='saga')\n",
        "model_W2V_RL = LogisticRegression(solver='saga')\n",
        "\n",
        "\n",
        "#Regression logistique CV\n",
        "model_DTM_RLCV = LogisticRegressionCV(penalty=\"l2\",solver='saga',cv=3,max_iter=5)\n",
        "model_TFID_RLCV = LogisticRegressionCV(penalty=\"l2\",solver='saga',cv=3,max_iter=5)\n",
        "model_W2V_RLCV = LogisticRegressionCV(penalty=\"l2\",solver='saga',cv=3,max_iter=5)\n",
        "\n",
        "\n",
        "#Naive Bayes\n",
        "model_DTM_NB = MultinomialNB(alpha=0.05)\n",
        "model_TFID_NB = MultinomialNB(alpha=0.007)\n",
        "\n",
        "\n",
        "#SVC\n",
        "model_DTM_SVC =LinearSVC(loss='hinge',multi_class='ovr')\n",
        "model_TFID_SVC =LinearSVC(loss='hinge',multi_class='ovr')\n",
        "model_W2V_SVC =LinearSVC(loss='hinge',multi_class='ovr')\n",
        "\n",
        "\n",
        "#SGDC\n",
        "model_DTM_SGDC=SGDClassifier(loss='hinge', penalty='l2', alpha=0.00001, l1_ratio=0.85, fit_intercept=True, max_iter=1000,tol=0.001)\n",
        "model_TFID_SGDC=SGDClassifier(loss='hinge', penalty='l2', alpha=0.00001, l1_ratio=0.85, fit_intercept=True, max_iter=1000,tol=0.001)\n",
        "model_W2V_SGDC=SGDClassifier(loss='hinge', penalty='l2', alpha=0.00001, l1_ratio=0.85, fit_intercept=True, max_iter=1000,tol=0.001)\n",
        "\n",
        "\n",
        "#Forêts aléatoires\n",
        "model_DTM_RF =RandomForestClassifier(n_estimators=30,max_features=10000)\n",
        "model_TFID_RF =RandomForestClassifier(n_estimators=30,max_features=10000)\n",
        "model_W2V_RF =RandomForestClassifier(n_estimators=30,max_features=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB3pzTBQ3GvJ",
        "outputId": "88d49426-79b6-4a17-9e71-f4d6cd463006"
      },
      "source": [
        "#fitting\n",
        "model_DTM_RL.fit(X_train_split_DTM[0:nb,:], y_train_split_DTM[0:nb])\n",
        "model_TFID_RL.fit(X_train_split_TFID[0:nb,:], y_train_split_TFID[0:nb])\n",
        "model_W2V_RL.fit(X_train_split_W2V[0:nb,:], y_train_split_W2V[0:nb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apSe31-C3GvK",
        "outputId": "f10b1e9e-9d8a-4e7c-e300-28850ec1b4c8"
      },
      "source": [
        "model_DTM_RLCV.fit(X_train_split_DTM[0:nb,:], y_train_split_DTM[0:nb])\n",
        "model_TFID_RLCV.fit(X_train_split_TFID[0:nb,:], y_train_split_TFID[0:nb])\n",
        "model_W2V_RLCV.fit(X_train_split_W2V[0:nb,:], y_train_split_W2V[0:nb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n",
              "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
              "                     max_iter=5, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                     random_state=None, refit=True, scoring=None, solver='saga',\n",
              "                     tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRod8AIg3GvK",
        "outputId": "c3fb9aa6-cea8-4c48-e192-80f06cf0403b"
      },
      "source": [
        "model_DTM_NB.fit(X_train_split_DTM[0:nb,:], y_train_split_DTM[0:nb])\n",
        "model_TFID_NB.fit(X_train_split_TFID[0:nb,:], y_train_split_TFID[0:nb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=0.007, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDEoBD6a3GvL",
        "outputId": "0d644f66-893b-465c-ef80-18de85f4a41e"
      },
      "source": [
        "model_DTM_SVC.fit(X_train_split_DTM[0:nb,:], y_train_split_DTM[0:nb])\n",
        "model_TFID_SVC.fit(X_train_split_TFID[0:nb,:], y_train_split_TFID[0:nb])\n",
        "model_W2V_SVC.fit(X_train_split_W2V[0:nb,:], y_train_split_W2V[0:nb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "          penalty='l2', random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E7AEVrH3GvL",
        "outputId": "48e77297-7949-4b6f-ba4c-25d30234b5cd"
      },
      "source": [
        "model_DTM_SGDC.fit(X_train_split_DTM[0:nb,:], y_train_split_DTM[0:nb])\n",
        "model_TFID_SGDC.fit(X_train_split_TFID[0:nb,:], y_train_split_TFID[0:nb])\n",
        "model_W2V_SGDC.fit(X_train_split_W2V[0:nb,:], y_train_split_W2V[0:nb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=1e-05, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "              l1_ratio=0.85, learning_rate='optimal', loss='hinge',\n",
              "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
              "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
              "              validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g4wxalv3GvL",
        "outputId": "eb9ff22e-9eca-479d-9ba3-a7b82cb36721"
      },
      "source": [
        "model_DTM_RF.fit(X_train_split_DTM[0:nb,:], y_train_split_DTM[0:nb])\n",
        "model_TFID_RF.fit(X_train_split_TFID[0:nb,:], y_train_split_TFID[0:nb])\n",
        "model_W2V_RF.fit(X_train_split_W2V[0:nb,:], y_train_split_W2V[0:nb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features=20,\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=30,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivryz8Oc3GvL"
      },
      "source": [
        "#Les prédictions\n",
        "y_pred_split_DTM_RL = model_DTM_RL.predict(X_valid_split_DTM)\n",
        "y_pred_split_TFID_RL = model_TFID_RL.predict(X_valid_split_TFID)\n",
        "y_pred_split_W2V_RL = model_W2V_RL.predict(X_valid_split_W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCb-Hb0a3GvM"
      },
      "source": [
        "y_pred_split_DTM_RLCV = model_DTM_RLCV.predict(X_valid_split_DTM)\n",
        "y_pred_split_TFID_RLCV = model_TFID_RLCV.predict(X_valid_split_TFID)\n",
        "y_pred_split_W2V_RLCV = model_W2V_RLCV.predict(X_valid_split_W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR-U5DZ53GvM"
      },
      "source": [
        "y_pred_split_DTM_NB = model_DTM_NB.predict(X_valid_split_DTM)\n",
        "y_pred_split_TFID_NB = model_TFID_NB.predict(X_valid_split_TFID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sKFIJX53GvM"
      },
      "source": [
        "y_pred_split_DTM_SVC = model_DTM_SVC.predict(X_valid_split_DTM)\n",
        "y_pred_split_TFID_SVC = model_TFID_SVC.predict(X_valid_split_TFID)\n",
        "y_pred_split_W2V_SVC = model_W2V_SVC.predict(X_valid_split_W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoEfGMB13GvN"
      },
      "source": [
        "y_pred_split_DTM_SGDC = model_DTM_SGDC.predict(X_valid_split_DTM)\n",
        "y_pred_split_TFID_SGDC = model_TFID_SGDC.predict(X_valid_split_TFID)\n",
        "y_pred_split_W2V_SGDC = model_W2V_SGDC.predict(X_valid_split_W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zblp7Wr_3GvN"
      },
      "source": [
        "y_pred_split_DTM_RF = model_DTM_RF.predict(X_valid_split_DTM)\n",
        "y_pred_split_TFID_RF = model_TFID_RF.predict(X_valid_split_TFID)\n",
        "y_pred_split_W2V_RF = model_W2V_RF.predict(X_valid_split_W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtxVcPkd3GvN"
      },
      "source": [
        "# DTM\n",
        "DTM_RL=sum(np.array(y_pred_split_DTM_RL == y_valid_split_DTM,dtype=int))/len(y_valid_split_DTM)\n",
        "DTM_RLCV=sum(np.array(y_pred_split_DTM_RLCV == y_valid_split_DTM,dtype=int))/len(y_valid_split_DTM)\n",
        "DTM_NB=sum(np.array(y_pred_split_DTM_NB == y_valid_split_DTM,dtype=int))/len(y_valid_split_DTM)\n",
        "DTM_SVC=sum(np.array(y_pred_split_DTM_SVC == y_valid_split_DTM,dtype=int))/len(y_valid_split_DTM)\n",
        "DTM_SGDC=sum(np.array(y_pred_split_DTM_SGDC == y_valid_split_DTM,dtype=int))/len(y_valid_split_DTM)\n",
        "DTM_RF=sum(np.array(y_pred_split_DTM_RF == y_valid_split_DTM,dtype=int))/len(y_valid_split_DTM)\n",
        "\n",
        "# TFID\n",
        "TFID_RL=sum(np.array(y_pred_split_TFID_RL == y_valid_split_TFID,dtype=int))/len(y_valid_split_TFID)\n",
        "TFID_RLCV=sum(np.array(y_pred_split_TFID_RLCV == y_valid_split_TFID,dtype=int))/len(y_valid_split_TFID)\n",
        "TFID_NB=sum(np.array(y_pred_split_TFID_NB == y_valid_split_TFID,dtype=int))/len(y_valid_split_TFID)\n",
        "TFID_SVC=sum(np.array(y_pred_split_TFID_SVC == y_valid_split_TFID,dtype=int))/len(y_valid_split_TFID)\n",
        "TFID_SGDC=sum(np.array(y_pred_split_TFID_SGDC == y_valid_split_TFID,dtype=int))/len(y_valid_split_TFID)\n",
        "TFID_RF=sum(np.array(y_pred_split_TFID_RF == y_valid_split_TFID,dtype=int))/len(y_valid_split_TFID)\n",
        "\n",
        "#W2V\n",
        "W2V_RL=sum(np.array(y_pred_split_W2V_RL == y_valid_split_W2V,dtype=int))/len(y_valid_split_W2V)\n",
        "W2V_RLCV=sum(np.array(y_pred_split_W2V_RLCV == y_valid_split_W2V,dtype=int))/len(y_valid_split_W2V)\n",
        "W2V_SVC=sum(np.array(y_pred_split_W2V_SVC == y_valid_split_W2V,dtype=int))/len(y_valid_split_W2V)\n",
        "W2V_SGDC=sum(np.array(y_pred_split_W2V_SGDC == y_valid_split_W2V,dtype=int))/len(y_valid_split_W2V)\n",
        "W2V_RF=sum(np.array(y_pred_split_W2V_RF == y_valid_split_W2V,dtype=int))/len(y_valid_split_W2V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpBMP2ZS3GvN",
        "outputId": "ffe8fae7-5491-4f56-af93-48b4a24a13e2"
      },
      "source": [
        "d = {'Regression Logistique': [DTM_RL, TFID_RL,W2V_RL], 'Regression Logistique_CV': [DTM_RLCV, TFID_RLCV,W2V_RLCV], \n",
        "     'Naive Bayes': [DTM_NB, TFID_NB,0], 'SVC': [DTM_SVC, TFID_SVC,W2V_SVC], \n",
        "     'SGDC': [DTM_SGDC, TFID_SGDC,W2V_SGDC], 'Random Forest': [DTM_RF, TFID_RF,W2V_RF]}\n",
        "\n",
        "resultats = pd.DataFrame(data=d,\n",
        "                 index=['DTM', 'TF-IDF', 'W2V'])\n",
        "\n",
        "def color_negative_red(val):\n",
        "    if val>0.80:\n",
        "        color='blue'\n",
        "    elif val>0.75:\n",
        "        color=\"black\"\n",
        "    elif val==0:\n",
        "        color='white'\n",
        "    else:\n",
        "        color=\"red\"\n",
        "    return 'color: %s' % color\n",
        "\n",
        "resultats.style.applymap(color_negative_red)\n",
        "\n",
        "#df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col0 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col1 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col2 {\n",
              "            color:  red;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col3 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col4 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col5 {\n",
              "            color:  red;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col0 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col1 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col2 {\n",
              "            color:  red;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col3 {\n",
              "            color:  blue;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col4 {\n",
              "            color:  blue;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col5 {\n",
              "            color:  red;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col0 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col1 {\n",
              "            color:  black;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col2 {\n",
              "            color:  white;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col3 {\n",
              "            color:  red;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col4 {\n",
              "            color:  red;\n",
              "        }    #T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col5 {\n",
              "            color:  red;\n",
              "        }</style><table id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Regression Logistique</th>        <th class=\"col_heading level0 col1\" >Regression Logistique_CV</th>        <th class=\"col_heading level0 col2\" >Naive Bayes</th>        <th class=\"col_heading level0 col3\" >SVC</th>        <th class=\"col_heading level0 col4\" >SGDC</th>        <th class=\"col_heading level0 col5\" >Random Forest</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00level0_row0\" class=\"row_heading level0 row0\" >DTM</th>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col0\" class=\"data row0 col0\" >0.788352</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col1\" class=\"data row0 col1\" >0.792127</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col2\" class=\"data row0 col2\" >0.71415</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col3\" class=\"data row0 col3\" >0.755233</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col4\" class=\"data row0 col4\" >0.763674</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row0_col5\" class=\"data row0 col5\" >0.747176</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00level0_row1\" class=\"row_heading level0 row1\" >TF-IDF</th>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col0\" class=\"data row1 col0\" >0.791667</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col1\" class=\"data row1 col1\" >0.796532</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col2\" class=\"data row1 col2\" >0.705479</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col3\" class=\"data row1 col3\" >0.802118</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col4\" class=\"data row1 col4\" >0.801366</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row1_col5\" class=\"data row1 col5\" >0.74202</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00level0_row2\" class=\"row_heading level0 row2\" >W2V</th>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col0\" class=\"data row2 col0\" >0.756845</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col1\" class=\"data row2 col1\" >0.753852</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col2\" class=\"data row2 col2\" >0</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col3\" class=\"data row2 col3\" >0.554082</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col4\" class=\"data row2 col4\" >0.651872</td>\n",
              "                        <td id=\"T_51a0a8ca_55e8_11eb_880e_c0e434e9db00row2_col5\" class=\"data row2 col5\" >0.63453</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x2a137f90108>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Y09jcQ3GvN"
      },
      "source": [
        "<p style=\"text-align: justify;\">On peut observer de manière synthétique que le TF-IDF est pour toutes les méthodes ou presque la méthode la plus efficace. Les trois méthodes produisant les meilleurs résultats sont la régression logistique en validation croisée ainsi que SVC et SGDC. Il convient de mentionner que nous avons optimisé (de manière empirique) les différents paramètres des différentes fonctions hormis ceux de la forêts aléatoires qu'il est difficile de pousser plus loin pour des raisons de cout computationnel. Pour Naive Bayes Nous n'avons pas fait la technique W2V qui de toute manière ne semble pas intéressante.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Les méthodes SVC et SGDC produisent des résultats très proche car l'idée générale est proche, en réalité pour SGDC il s'agit d'un SVM avec la technique de la descente de gradient stochastique.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Lors des rendus des différents résultats sur le kaggle du defi IA, la regression logistique CV ainsi que les SVC et SGDC pouvait atteindre des scores supérieurs à 0.74 voir 0.75.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Notre meilleure soumission de ce côté-là a donc été SGDC qui a produit un score aux alentours de 0.75. Il convient ensuite de réentrainer le modèle sur toutes les données d'entrainement, de prédire sur les données test et d'enregistrer les résultats sous la forme d'un csv, ce que nous ferons plutôt dans la partie suivante car toutes ces méthodes ne produisent pas des résultats aussi bon que la suivante : BERT</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMwnrU0V3GvN"
      },
      "source": [
        "<u> <h2><FONT color=\"darkred\"> IV)2 - BERT & ROBERTA </FONT></h2> </u>\n",
        "\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/max/2400/0*x3vhaoJdGndvZqmL.png\" alt=\"exemple de texte alternatif\" width=300px /></center>\n",
        "\n",
        "<p style=\"text-align: justify;\">Désormais nous allons nous intéresser à un algorithme assez récent qui est réputé pour avoir été la plus grande percée dans le domaine des NLP depuis plusieurs années et produire les meilleurs résultats.</p>\n",
        "    \n",
        "<p style=\"text-align: justify;\">Dans le cadre de l'UE \"Lecture d'articles et de documents scientifiques\", nous avons eu à réaliser une synthèse d'un article scientifique. Notre groupe a choisi de s'intéresser à BERT et en particulier à TANDA, la dernière avancée concernant BERT dans le domaine QA : Questions-réponses, le type d'algorithmes utilisé dans les enceintes connectées d'Amazon et Google. Ici, le problème n'est pas exactement le même mais nous mettons à disposition notre synthèse de 4 pages sur le sujet dont une grosse première partie (page 1 pour les pré-requis et page 2 pour BERT) explique de manière détaillé le principe de fonctionnement de BERT :</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFZZZYjf3GvO"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> Notre synthèse :</FONT></h3></u>\n",
        "\n",
        "<img\n",
        "    src=\"./GUERIN_LABORDE_TANDA_LADS-1.jpg\"\n",
        "    style=\"border:solid 2px #000000;\"\n",
        "    alt=\"GUERIN_LABORDE_BERT\"\n",
        "    height=\"1000px\"\n",
        "    width=\"800px\"\n",
        "/>\n",
        "<img\n",
        "    src=\"./GUERIN_LABORDE_TANDA_LADS-2.jpg\"\n",
        "    style=\"border:solid 2px #000000;\"\n",
        "    alt=\"GUERIN_LABORDE_BERT\"\n",
        "    height=\"1000px\"\n",
        "    width=\"800px\"\n",
        "/>\n",
        "<img\n",
        "    src=\"./GUERIN_LABORDE_TANDA_LADS-3.jpg\"\n",
        "    style=\"border:solid 2px #000000;\"\n",
        "    alt=\"GUERIN_LABORDE_BERT\"\n",
        "    height=\"1000px\"\n",
        "    width=\"800px\"\n",
        "/>\n",
        "<img\n",
        "    src=\"./GUERIN_LABORDE_TANDA_LADS-4.jpg\"\n",
        "    style=\"border:solid 2px #000000;\"\n",
        "    alt=\"GUERIN_LABORDE_BERT\"\n",
        "    height=\"1000px\"\n",
        "    width=\"800px\"\n",
        "/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "M_HP3Hm_3GvO"
      },
      "source": [
        "#from IPython.display import IFrame\n",
        "#IFrame(\"./GUERIN_LABORDE_TANDA_LADS.pdf\", width=900, height=3720)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dplzYQHi3GvO"
      },
      "source": [
        "<p style=\"text-align: justify;\">Cependant l'implémentation de cette tache particulière pour BERT est totalement différente de tout ce que l'on avait pu voir jusqu'à présent et a été difficile à adapter, nous n'avons réussi que vers la fin du projet.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Nous allons effectuer un résumé rapide de ce en quoi consiste BERT. Il s'agit d'une version particulière des Transformers. Les Transformers sont des réseaux de neurones qui prennent en entrée les Embeddings d'une phrase (très similaire aux words 2 Vec) et agissent dessus en effectuant des couches de neurones de type \"add&Norm\", \"feed forward\" et \"multi-head attention\" (expliqué en détails en p.2 de notre synthèse). Ensuite via des fonctions de sorties particulières les Transformers sont capables calculer des probabilités pour une phrase donnée d'appartenir à telle ou telle classe.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">BERT est ainsi une version particulière des Transformers auxquels on aurait retiré la partie \"Decoding\". Etant donné la complexité de l'entrainement de BERT et de la quantité de données nécessaires seules quelques entreprises et instituts dans le monde peuvent se le permettre, en effet BERT a été entrainé sur 16 Gigas de de données en texte brut et a couté plus de 250 000 dollars. Des versions améliorées de BERT existent et sont entrainés sur encore plus de données comme BERT-Large, XL-net, RoBERTa, RoBERTa-Large, etc... .</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Dans les faits, on se sert donc en général de ces modèles pré-entrainés et on applique ce qu'on appele un fine-tuning qui consiste à adapter nos modèles pré-entrainés à nos données particulières. C'est ce que nous faisons dans la suite en adaptant un modèle pré-entrainé de BERT, dans sa version adapté à la classification de phrases (BERTforSequenceClassification), à nos descriptions sur des personnes afin de trouver le métier y correspondant.</p>\n",
        "\n",
        "<p style=\"text-align: justify;\">Nous utilisons ici deux modèles différents BERT-base et Roberta-base.</p>\n",
        "\n",
        "\n",
        "<h3>Point d'attention !</h3>\n",
        "<p style=\"text-align: justify;\">Il convient de noter que ce code ne peut pas vraiment être lancé sur une machine locale personnelle, nous l'avons exécuté depuis Google Colab en utilisant les cartes graphiques professionelles qu'ils mettent à disposition. Les sorties ne sont donc pas présentes ici mais nous joignons un .ipynb sur lesquelles elles le sont en plus de ce notebook.</p>\n",
        "\n",
        "<h3>Deuxième Point d'attention !</h3>\n",
        "<p style=\"text-align: justify;\">Ce code a été adapté de celui d'une personne/site web qui est cité dans la bibliographie. Nous avons adapté celui-ci à nos données et à notre type de problème, rajouté une version pour utiliser RoBERTa et rajouté tous les commentaires de codes en français afin qu'il soit plus compréhensible.</p>\n",
        "\n",
        "Ci-dessous le sommaire de cette partie afin de s'y retrouver :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUV74px3GvO"
      },
      "source": [
        "<h2><u>Sommaire :</u></h2>\n",
        "<ul>\n",
        "  <li> Importation </li>\n",
        "  <li> Configuration des données </li>\n",
        "  <li> Configuration du modèle </li>\n",
        "  <li> Entrainement du modèle </li>\n",
        "  <li> Prédictions </li>\n",
        "  <li> Comparaison des prédictions de différents modèles </li>\n",
        "  <li> Bibliographie </li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt1f92db3GvP"
      },
      "source": [
        "<u> <h2><FONT color=\"darkred\"> I - Importation </FONT></h2> </u>\n",
        "\n",
        "<u><h3><FONT color=\"darkgreen\"> A - Importation des packages </FONT></h3></u>\n",
        "\n",
        "Nous installons les packages manquants transformers et wget. Puis il faut **importer** tous **les packages** pour la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlgFMB0A3GvP"
      },
      "source": [
        "#Installation \n",
        "!pip install transformers\n",
        "!pip install wget\n",
        "\n",
        "#Import \n",
        "import pandas as pd\n",
        "import wget\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import plotly.express as px\n",
        "\n",
        "#FROM\n",
        "from google.colab import drive\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBTEcTWZ3GvP"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> B - Connexion Google Drive </FONT></h3></u>\n",
        "\n",
        "Pour importer les données plus rapidement, nous importons un **google drive** qui comporte les données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QqFBzjx3GvP"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNdKSdqB3GvP"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> C - Importation des données </FONT></h3></u>\n",
        "\n",
        "Nous importons les données d'apprentissage grâce au fichier **train.json** qui comporte notamment les descriptions des métiers et grâce au fichier **train_label.csv** qui comporte les labels des métiers correspondant. Nous importons les données test grâce au fichier **test.json** comportant les descriptions test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5G8Jhe23GvP"
      },
      "source": [
        "DATA_PATH = \"./\"\n",
        "train_df = pd.read_json(DATA_PATH+\"/train.json\")\n",
        "test_df = pd.read_json(DATA_PATH+\"/test.json\")\n",
        "train_label = pd.read_csv(DATA_PATH+\"/train_label.csv\")\n",
        "\n",
        "train_df[\"description_lower\"] = [x.lower() for x in train_df.description]\n",
        "test_df[\"description_lower\"] = [x.lower() for x in test_df.description]\n",
        "\n",
        "sentences=train_df[\"description\"].values\n",
        "labels=train_label.Category.values\n",
        "#print(sentences)\n",
        "#print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4sdjmQW3GvP"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> D - Visualisation des données </FONT></h3></u>\n",
        "\n",
        "Nous visualisons ci-dessous une partie des données présentes dans le jeu de données d'entrainement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwRHsGrP3GvP"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYoORYzM3GvQ"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> E - GPU </FONT></h3></u>\n",
        "\n",
        "Nous demandons un GPU dans cette partie. Au préalable il faut aller dans :\n",
        "<fieldset><center>\n",
        " <strong> Modifier - Paramètres du Notebook - Accélérateur matériel => GPU </strong></center>\n",
        " </fieldset>\n",
        " \n",
        "  Si aucun est disponible on utilise le CPU à la place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI7MIasu3GvQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQEk3Ay13GvQ"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58qSbl0D3GvQ"
      },
      "source": [
        "Il faut prendre le GPU le plus puissant qui est ici le **Tesla P100-PCIE-16GB** pour plus de rapidité. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X7vmtug3GvQ"
      },
      "source": [
        "<u><h2><FONT color=\"darkred\"> II - Préparation des données </FONT></h2></u>\n",
        "\n",
        "Avant de passer aux configurations de nos modèles il est important de configurer nos données d'entrée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJAfFuEY3GvR"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> A - Chargement Tokenizer </FONT></h3></u>\n",
        "\n",
        "<p> Il faut choisir un des deux Tokenizer correspondant au modèle que l'on souhaite utiliser par la suite. Il existe plusieurs librairies pour chaque tokenizer.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH3S_8Y03GvS"
      },
      "source": [
        "<h4> 1 - Roberta Tokenizer For Sequence Classification </h4>\n",
        "\n",
        "Nous utilisons ici **RobertaTokenizer** avec la librairie **\"roberta-base\"**. Il en existe d'autres consultable sur le site huggingface. Nous aurions pu utiliser roberta large qui est entrainée sur plus de mots seulement pour des problèmes de coût computationnel il n'était pas possible de l'exécuter sur Google Colab avec ce jeu de données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tz7dnNv3GvS"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDM-mrzy3GvS"
      },
      "source": [
        "<h4> 2 - Bert Tokenizer For Sequence Classification </h4>\n",
        "\n",
        "Nous utilisons ici **BertTokenizer** avec la librairie **\"bert-base-uncased\"**. Nous aurions pu utiliser bert large qui est entrainé sur plus de mots mais pour les mêmes raisons que précedemment il n'était pas possible de l'exécuter sur Google Colab avec ce jeu de données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_XDiUtO3GvS"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zho9gHjm3GvS"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> B - Tokenization </FONT></h3></u>\n",
        "\n",
        "Nous allons tokenizer les phrases c'est-à-dire réaliser un découpage par mot et dans un second temps affecter des ID à chaque mot. Comme on l'a vu précédemment il existe plusieurs techniques de tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK5pBIR13GvS"
      },
      "source": [
        "#Tokenization de toutes les phrases et affectation de l'id pour chaque mots \n",
        "input_ids = []\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        #max_length = 128,          # Tronquez toutes les phrases.\n",
        "                        #return_tensors = 'pt',     # Renvoie pytorch tensors\n",
        "                   )\n",
        "    \n",
        "    #Ajout de la phrase encoded à la liste inputs_ids\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "#Affichage de la première phrase et de la liste des id\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzJ7xGXu3GvT"
      },
      "source": [
        "print('La phrase la plus longue possède : ', max([len(sen) for sen in input_ids]), ' caractères' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvvHSn5Y3GvT"
      },
      "source": [
        "Sur le chunk suivant nous allons choisir la **longueur maximale en caractère des phrases** de l'entrainement. Pour des raisons computationnelles nous choissions **486 caractères** car au-delà de cette valeur nous rencontrons des problèmes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzvB2Mq23GvT"
      },
      "source": [
        "MAX_LEN = 486\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Remplir nos tokens d'entrée avec la valeur 0.\n",
        "# \"post\" indique que nous voulons remplir et tronquer à la fin de la séquence, par opposition au début.\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print('\\Done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5n931pD3GvT"
      },
      "source": [
        "Nous allons créer des **masques d'attention**. Le masque d'attention indique simplement quels tokens sont des mots réels par rapport à ceux qui ne le sont pas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1PVdIlm3GvT"
      },
      "source": [
        "# Création du \"attention masque\" pour toutes les phrases.\n",
        "attention_masks = []\n",
        "for sent in input_ids:\n",
        "\n",
        "    # - Si un token ID est 0, alors c'est un padding, définissez le masque sur 0.\n",
        "    # - Si un token ID est> 0, alors c'est un vrai token, définissez le masque sur 1.\n",
        "\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Ajout du \"attention mask\" de la phrase\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqz3DpHC3GvT"
      },
      "source": [
        "On réalise un **découpage apprentissage/validation** pour les inputs/labels et pour les masques pour l'entrainement avec comme proportion **90%** pour l'apprentissage et **10%** pour la validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhW-eh1T3GvU"
      },
      "source": [
        "# Inputs/Labels\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Train/Validation\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka54xRMK3GvU"
      },
      "source": [
        "On convertit les inputs/labels et masks en **torch tensors**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDTa5_FD3GvU"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou3rzq6Y3GvV"
      },
      "source": [
        "\n",
        "Dans le chunk ci-dessous on choisit la **taille des batchs**, les valeurs recommandées pour le fine-tuning pour des taches spécifiques de ce type de modèle Bert ou Roberta sont de **16 ou 32**. Nous avons opté pour des batchs de taille 16 car malgré le fait que cette taille augmente le temps d'exécution elle permet de prendre une longueur maximale des phrases en caractères plus grande. Nous allons également créer un **itérateur** pour notre ensemble de données à l'aide de la classe **Torch DataLoader**. Cela permet d'économiser de la mémoire pendant l'entraînement car, contrairement à une boucle for, avec un itérateur, l'ensemble de données n'a pas besoin d'être chargé en mémoire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJiTYZLP3GvV"
      },
      "source": [
        "# Choix batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Créez le DataLoader pour notre ensemble d'apprentissage.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Créez le DataLoader pour notre ensemble de validation.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftt31mR73GvV"
      },
      "source": [
        "<u><h2><FONT color=\"darkred\"> III - Configuration du modèle </FONT></h2> </u>\n",
        "\n",
        "Nous allons dans cette partie configurer notre modèle. Il convient de choisir un des deux modèles possible en cohérence avec le tokenizer choisi en amont."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfraDyQ93GvW"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> A - Choix du modèle </FONT></h3></u>\n",
        "\n",
        "<h4> 1 - Modèle RobertaForSequenceClassification </h4>\n",
        "\n",
        "Nous configurons le modèle avec le paramètre **num_labels=28** qui correspond au nombre de labels dans la variable a expliquer qui est de 28 dans notre cas. Notre modèle pré-entrainé avec la librairie **roberta-base** comme nous l'avons mentionné précédemment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6cKx-_r3GvW"
      },
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    'roberta-base',\n",
        "    num_labels=28,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2vj_aDy3GvW"
      },
      "source": [
        "<h4> 2 - Modèle BertForSequenceClassification </h4>\n",
        "\n",
        "Nous prenons le même **num_labels**. Ici la librairie de pré-traitement est **bert-base-uncased**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aet9yyV_3GvW"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 28,    \n",
        "    output_attentions = False, # Affichage du modèle des attentions poids  \n",
        "    output_hidden_states = False, # Affichage du modèle de toutes les couches cachés\n",
        ")\n",
        "# Indique à pytorch d'executer le modèle sur le GPU\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnZt-4_p3GvW"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> B - Visualisation des paramètres du modèles </FONT></h3></u>\n",
        "\n",
        "Nous visualisons certains paramètres de notre modèle notamment sur les Embedding Layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76XIBwwN3GvX"
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ6fYG-a3GvX"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> C - Optimisation AdamW </FONT></h3></u>\n",
        "\n",
        "Dans le chunk suivant nous optimisons les paramètres de notre modèle avec AdamW. Nous choisissons  également le nombre d'epochs (Nombre de fois que l'on étudie chaque donnée)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoV_ZbvJ3GvX"
      },
      "source": [
        "from transformers import AdamW\n",
        "# Note: AdawW est une classe venant de librarie d'hugging face (opposé à pytorch)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - par défaut 5e-5, ici 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - par défaut 1e-8.\n",
        "                )\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Nombre d'epoch pour le training (entre 2 et 4)\n",
        "epochs = 3\n",
        "\n",
        "# Nombre total d'étapes : nombre de batchs * nombre d'epochs\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Créez le planificateur de taux d'apprentissage.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjIttOW13GvX"
      },
      "source": [
        "<u><h2><FONT color=\"darkred\"> IV - Entrainement du modèle sur le train et validation </FONT></h2></u>\n",
        "\n",
        "Nous allons entrainer notre modèle sur la partie train et verifier sur la partie validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EcS9V2z3GvX"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> A - Définition de fonction </FONT></h3></u>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lI5YYjA3GvY"
      },
      "source": [
        "<h5><strong> flat accuracy </strong></h5>\n",
        "\n",
        "Nous implémentons une fonction pour calculer la précision de nos prédictions par rapport aux labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrlgCdRr3GvY"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPjRd9vP3GvY"
      },
      "source": [
        "<h5> <strong>format time</strong> </h5>\n",
        "\n",
        "Nous allons implémenter une fonction pour calculer le temps d'exécution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5oCrvVs3GvY"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    # Arrondir à la seconde la plus proche\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format hh:mm:ss ce qui correspond à Heure:Minute:Seconde\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0U6QyFM3GvY"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> B - Entrainement </FONT></h3></u>\n",
        "\n",
        "Ci-dessous le chunk d'entrainement et de validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbLCGGv03GvZ"
      },
      "source": [
        "# Initialisation d'un seed pour avoir des résultats reproductibles\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Stockez la perte moyenne après chaque époque afin de pouvoir les tracer.\n",
        "loss_values = []\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Apprentissage\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Mesure du temps écoulé pour l'apprentisagge d'une epoch\n",
        "    t0 = time.time()\n",
        "    \n",
        "    # Réinitialisez la loss totale pour cette époque.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Model en training mode\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progrès mis à jour tous les 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            \n",
        "            # Calculez le temps écoulé en minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Affichage de la progression\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "\n",
        "        # Sortir le batch training de notre dataloader \n",
        "\n",
        "        # Au fur et à mesure que nous déballons(\"unpack\") le batch, \n",
        "        # nous copierons également chaque tenseur sur le GPU en utilisant la méthode `to`.\n",
        "        \n",
        "        # Batchs contiennent trois pytorch tensors :\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Effacez toujours les gradients précédemment calculés avant d'effectuer une backward pass. \n",
        "        # PyTorch ne le fait pas automatiquement car l'accumulation des gradients est \"pratique lors de l'entrainement des RNN\".\n",
        "\n",
        "        model.zero_grad()  \n",
        "\n",
        "        # Effectuer une forward pass (évaluer le modèle sur ce batch d'entraînement).\n",
        "        # Cela renverra la perte/loss (plutôt que la sortie du modèle) car nous avons fourni les `labels`.\n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # L'appel à `model` retourne toujours un tuple, \n",
        "        # nous devons donc extraire la valeur de perte/loss du tuple.\n",
        "\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        # Accumuler la perte/loss d'entraînement sur tous les batchs afin de pouvoir calculer la perte/loss moyenne à la fin.\n",
        "        # «loss» est un Tensor contenant une valeur unique; \n",
        "        # la fonction `.item ()` renvoie simplement la valeur Python du tenseur.\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Effectuez une backward pass pour calculer les gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Fixez la norme des gradients à 1.0.\n",
        "        # Ceci permet d'éviter le problème des \"exploding gradients\".\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Mettez à jour les paramètres et faites un pas en utilisant le gradient calculé.\n",
        "\n",
        "        # L'optimiseur dicte la \"règle de mise à jour\" \n",
        "        # comment les paramètres sont modifiés en fonction de leurs gradients, du taux d'apprentissage, etc.\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Mis à jour du taux d'apprentissage(\"learning rate\")\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculez la perte moyenne sur les données d'entraînement. \n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Stockez la valeur de perte pour tracer la courbe d'apprentissage.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    # Après chaque epoch de training, mesure de  notre performance sur\n",
        "    # notre ensemble de validation.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Mettre le modèle en mode évaluation\n",
        "    model.eval()\n",
        "\n",
        "    # Suivre variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Suivre variables \n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        #Ajout batch au GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Sortir le batch training de notre dataloader \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Dire au modèle de ne pas calculer ou stocker les gradients,\n",
        "        # économiser de la mémoire et accélérer la validation\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Calculer les logits\n",
        "\n",
        "            # Cela renverra les logits plutôt que la perte car nous n'avons pas fourni d'étiquettes.\n",
        "\n",
        "            # token_type_ids est le même que les \"segment ids\", \n",
        "            # qui différencient les phrases 1 et 2 dans les tâches à 2 phrases.\n",
        "\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "            \n",
        "        # Prendre les logits sortis par le modèle    \n",
        "        logits = outputs[0]\n",
        "        \n",
        "        # Les \"logits\" sont les valeurs de sortie avant d'appliquer une fonction d'activation comme le softmax.\n",
        "\n",
        "        # Déplacer les logits et les labels vers le CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculez la précision  pour ce batch pour les phrases test.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "  \n",
        "        # Somme pour la précision totale\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Suivre/Actualiser le nombre de batch\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "         \n",
        "    # Affichage de la précision finale de ce validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLY7oHG-3GvZ"
      },
      "source": [
        "Nous avons réaliser un graphique pour observer la **perte/loss d'entrainement** du modèle en fonction des epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cei0C8L3GvZ"
      },
      "source": [
        "f = pd.DataFrame(loss_values)\n",
        "f.columns=['Loss']\n",
        "fig = px.line(f, x=f.index, y=f.Loss)\n",
        "fig.update_layout(title='Training loss of the Model',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDNvkx_q3GvZ"
      },
      "source": [
        "Il est possible de rajouter certaines variables pour visualiser la présence ou non de **surapprentissage** ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIY5mb-h3Gva"
      },
      "source": [
        "<u><h2><FONT color=\"darkred\"> V - Prédictions </FONT></h2></u>\n",
        "\n",
        "Maintenant que nous avons configuré, optimisé et entrainé notre modèle nous allons **prédire les labels** avec le jeu de données **test**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLrKI90r3Gva"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> A - Configuration données test </FONT></h3></u>\n",
        "\n",
        "Tout d'abord il convient de configurer le jeu de données test comme le jeu de données train (Tokenization, Masque, Torch Tensor, Batch Size, DataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acd2W7qG3Gva"
      },
      "source": [
        "# Définition de sentences/labels\n",
        "sentences=test_df[\"description\"].values\n",
        "labels=train_label[0:54300].Category.values\n",
        "\n",
        "# Affichage du nombre de phrase\n",
        "print('Le jeu de données test comporte : {:,}\\n'.format(test_df.shape[0]),\"phrase\")\n",
        "\n",
        "# Tokenization du jeu de données test\n",
        "input_ids = []\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad_sequences : réglages de la longueur maximale\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Attention masks\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convertion en torch tensor\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Choix du batch size.  \n",
        "batch_size = 32 \n",
        "\n",
        "# Créer DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a862Oys43Gva"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> B - Prédictions du jeu de données Test </FONT></h3></u>\n",
        "\n",
        "Nous allons à l'aide du modèle construire/prédire les **logits** des données test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2ZeW2bm3Gvb"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "# Model en mode eval\n",
        "model.eval()\n",
        "\n",
        "# variables \n",
        "predictions , true_labels = [], []\n",
        "# predictions\n",
        "for batch in prediction_dataloader:\n",
        "\n",
        "  # Ajout batch au GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Dire au modèle de ne pas calculer ou stocker les gradients, économiser de la mémoire et accélérer la prédiction \n",
        "  with torch.no_grad():\n",
        "      # calculer les logits de prédiction.\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Déplacer les logits et labels vers CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Stockage des prédictions et des true labels \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "print('DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ0ftk83Gvb"
      },
      "source": [
        "\n",
        "\n",
        "Pour construire le vecteur de prédictions il faut garder le **label** correspand au **logits maximal** pour chaque observations. Le tableau est un double tableau de taille **1697*32** ici. Le 32 correspond à la taille des batchs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz3z4g2e3Gvb"
      },
      "source": [
        "vect=[]\n",
        "for j in range(1696):\n",
        "  for i in range(32):\n",
        "    pred_labels = np.argmax(predictions[j][i], axis=0).flatten()\n",
        "    vect.append(pred_labels[0])\n",
        "for t in range(28):\n",
        "  pred_labels = np.argmax(predictions[1696][t], axis=0).flatten()\n",
        "  vect.append(pred_labels[0])\n",
        "  \n",
        "print(\"Le vecteur possède est:\",len(vect))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMoqVfpJ3Gvb"
      },
      "source": [
        "Nous stockons nos prédictions dans un csv avec deux colonnes : la première correspondant à l'identifiant de l'observation elle s'appelle **Id** et la deuxième correspond aux labels prédit qui a pour nom **Category**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVdL0AAt3Gvb"
      },
      "source": [
        "test_df[\"Category\"] = vect\n",
        "baseline_file = test_df[[\"Id\",\"Category\"]]\n",
        "baseline_file.to_csv(\"BERTAouf486_epoch2.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j63H8D-a3Gvb"
      },
      "source": [
        "<u><h2><FONT color=\"darkred\"> VI - Résultats </FONT></h2></u>\n",
        "\n",
        "<p style=\"text-align: justify;\">A l'aide du modèle pré-entrainé sur BERT-base en effectuant deux epochs avec un max_len de 486 et tous les paramètres définis ci-dessus nous obtenons un score de 0.816 sur le defi IA. Les mêmes paramètres pour RoBERTa-base produisent le même résultat alors que l'on s'attendait à légèrement mieux. Cela provient du fait que Roberta tokenize les phrases d'une manière légèrement différente et qui prend plus de place. La limite de 486 caractères a donc été plus limitante pour roberta que pour BERT ce qui explique le résultat. Il existe des techniques pour éviter cela mais nous n'avons pas eu le temps de s'y intéresser.</p>\n",
        "    \n",
        "<p style=\"text-align: justify;\">Une solution aurait été de prendre un abonnement colab Pro. De plus en poussant le nombre d'epochs à 3 et en prenant le modèle pré entrainé de RoBERTa-Large qui est le modèle remportant tous les defis de NLP a l'heure actuelle nous pensons que nous aurions pu obtenir un meilleur score encore. Pour terminer il convient aussi de mentionner que l'entrainement du modèle à pris environ 8 heures avec une carte graphique professionelle tesla P100 et que la limite de temps d'une session sur colab est de 12 heures. Nous ne pouvions donc pas faire beaucoup plus hormis en passant à la version pro de colab.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5l1A1tk3Gvb"
      },
      "source": [
        "<u><h2><FONT color=\"darkred\"> VII - Comparaison des prédictions pour différents modèles </FONT></h2></u>\n",
        "\n",
        "Il est possible de comparer les modèles sur leurs prédictions ce qui permet de voir quels sont les modèles les plus proches et permet de donner une première idée du taux d'erreur possible en fonction de la proximité avec d'autre modèles où l'erreur de prédiction est connue.\n",
        "\n",
        "<u><h3><FONT color=\"darkgreen\"> A - Chargement des csv de prédictions </FONT></h3></u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGv3auib3Gvb"
      },
      "source": [
        "DATA_PATH = \"./\"\n",
        "\n",
        "# ROBERT\n",
        "\n",
        "# epoch = 1\n",
        "\n",
        "roberta480 = pd.read_csv(DATA_PATH+\"/ROBERTAouf480.csv\")\n",
        "roberta486 = pd.read_csv(DATA_PATH+\"/ROBERTAouf486.csv\")\n",
        "roberta320 = pd.read_csv(DATA_PATH+\"/ROBERTAouf320.csv\")\n",
        "\n",
        "# epoch = 2\n",
        "\n",
        "roberta484E2 = pd.read_csv(DATA_PATH+\"/ROBERTAouf484_Epoch2.csv\")\n",
        "robert_2e = pd.read_csv(DATA_PATH+\"/ROBERTAouf484_Epoch2.csv\")\n",
        "\n",
        "# BERT\n",
        "\n",
        "# epoch = 1\n",
        "bert480 = pd.read_csv(DATA_PATH+\"/BERTouf_480.csv\")\n",
        "bert_480 = pd.read_csv(DATA_PATH+\"/BERTouf_480.csv\")\n",
        "\n",
        "# epoch = 2\n",
        "bert_2e = pd.read_csv(DATA_PATH+\"/BERTouf486_e2.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-LQZUVQ3Gvb"
      },
      "source": [
        "<u><h3><FONT color=\"darkgreen\"> B - Tableau du pourcentage de similitudes </FONT></h3></u>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AuGaqce3Gvc"
      },
      "source": [
        "print(sum(roberta480[\"Category\"]==roberta484E2[\"Category\"])/54300)\n",
        "print(sum(roberta486[\"Category\"]==roberta484E2[\"Category\"])/54300)\n",
        "\n",
        "print(sum(bert_2e[\"Category\"]==bert_480[\"Category\"])/54300)\n",
        "print(sum(robert_2e[\"Category\"]==bert_2e[\"Category\"])/54300)\n",
        "print(sum(robert_2e[\"Category\"]==bert_480[\"Category\"])/54300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mytmbo9p3Gvc"
      },
      "source": [
        "tot_=sum(roberta480[\"Category\"]==roberta486[\"Category\"])/54300\n",
        "tot_diff2=sum(roberta320[\"Category\"]==roberta486[\"Category\"])/54300\n",
        "tot_diff3=sum(bert480[\"Category\"]==roberta486[\"Category\"])/54300\n",
        "print(tot_)\n",
        "print(tot_diff2)\n",
        "print(tot_diff3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC-8uS543Gvc"
      },
      "source": [
        "tot_diff=sum(roberta480[\"Category\"]==roberta320[\"Category\"])/54300\n",
        "tot_diff3=sum(roberta480[\"Category\"]==bert480[\"Category\"])/54300\n",
        "print(tot_diff)\n",
        "print(tot_diff3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agbd9P2J3Gvc"
      },
      "source": [
        "rob_epoch1=sum(roberta480[\"Category\"]==roberta320[\"Category\"])/54300\n",
        "rob_epoch2=sum(roberta480[\"Category\"]==bert480[\"Category\"])/54300\n",
        "rob_epoch3=sum(roberta320[\"Category\"]==bert480[\"Category\"])/54300\n",
        "print(rob_epoch1)\n",
        "print(rob_epoch2)\n",
        "print(rob_epoch3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AucdYJmC3Gvd"
      },
      "source": [
        "<u><h2><FONT color=\"darkred\"> VIII - Bibliographie </FONT></h2></u>\n",
        "\n",
        "<ul>\n",
        "  <li> https://towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646  </li>\n",
        "\n",
        "  <li>https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1 </li>\n",
        "  \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDW1lc3w3Gvd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}